{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClinVar Pathogenicity Evaluation\n",
    "\n",
    "This notebook evaluates the multitask CRISPR design model on held-out ClinVar test data for variant pathogenicity classification.\n",
    "\n",
    "## Objectives\n",
    "1. Load trained model checkpoint\n",
    "2. Evaluate on held-out test set\n",
    "3. Compute classification metrics (AUROC, AUPRC, F1, accuracy)\n",
    "4. Plot ROC and PR curves\n",
    "5. Analyze confusion matrix and misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "from crispr_design_agent.evaluation.metrics import (\n",
    "    compute_classification_metrics,\n",
    "    compute_pr_curve_data,\n",
    "    compute_roc_curve_data,\n",
    ")\n",
    "from crispr_design_agent.evaluation.visualization import (\n",
    "    plot_calibration_curve,\n",
    "    plot_confusion_matrix,\n",
    "    plot_pr_curve,\n",
    "    plot_roc_curve,\n",
    ")\n",
    "from crispr_design_agent.training.module import MultiTaskLightningModule\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"../models/checkpoints/multitask-epoch=10.ckpt\"\n",
    "DATA_PATH = \"../data/processed/clinvar.parquet\"\n",
    "VAL_SPLIT = 0.1\n",
    "SEED = 42\n",
    "MAX_LENGTH = 1024\n",
    "THRESHOLD = 0.5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(DATA_PATH)\n",
    "print(f\"Loaded {len(df)} ClinVar variants\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df[\"is_pathogenic\"].value_counts())\n",
    "print(f\"\\nPathogenic ratio: {df['is_pathogenic'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/val\n",
    "df = df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
    "cutoff = int(len(df) * (1 - VAL_SPLIT))\n",
    "test_df = df.iloc[cutoff:].reset_index(drop=True)\n",
    "\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "print(f\"Test pathogenic ratio: {test_df['is_pathogenic'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "model = MultiTaskLightningModule.load_from_checkpoint(CHECKPOINT_PATH, strict=False)\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model.encoder_name, trust_remote_code=True)\n",
    "\n",
    "print(f\"Model loaded from {CHECKPOINT_PATH}\")\n",
    "print(f\"Encoder: {model.encoder_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_proba = []\n",
    "batch_size = 8\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i in range(0, len(test_df), batch_size):\n",
    "        batch_df = test_df.iloc[i : i + batch_size]\n",
    "        sequences = batch_df[\"sequence\"].tolist()\n",
    "        \n",
    "        tokens = tokenizer(\n",
    "            sequences,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            padding=\"max_length\",\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        pooled = model.forward(tokens[\"input_ids\"], tokens[\"attention_mask\"])\n",
    "        logits = model.heads[\"clinvar\"](pooled).squeeze(-1)\n",
    "        proba = torch.sigmoid(logits)\n",
    "        \n",
    "        predictions_proba.extend(proba.cpu().numpy().tolist())\n",
    "        \n",
    "        if (i + batch_size) % 100 == 0:\n",
    "            print(f\"Processed {min(i + batch_size, len(test_df))} / {len(test_df)}\")\n",
    "\n",
    "test_df[\"prediction_proba\"] = predictions_proba\n",
    "test_df[\"prediction\"] = (test_df[\"prediction_proba\"] >= THRESHOLD).astype(int)\n",
    "print(f\"\\nInference complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_df[\"is_pathogenic\"].values\n",
    "y_pred_proba = test_df[\"prediction_proba\"].values\n",
    "\n",
    "metrics = compute_classification_metrics(\n",
    "    y_true,\n",
    "    y_pred_proba,\n",
    "    threshold=THRESHOLD,\n",
    ")\n",
    "\n",
    "print(\"\\n=== ClinVar Classification Metrics ===\")\n",
    "for key, value in metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key:20s}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{key:20s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_data = compute_roc_curve_data(y_true, y_pred_proba)\n",
    "\n",
    "fig = plot_roc_curve(\n",
    "    roc_data[\"fpr\"],\n",
    "    roc_data[\"tpr\"],\n",
    "    roc_data[\"auroc\"],\n",
    "    title=\"ClinVar Pathogenicity - ROC Curve\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_data = compute_pr_curve_data(y_true, y_pred_proba)\n",
    "\n",
    "fig = plot_pr_curve(\n",
    "    pr_data[\"precision\"],\n",
    "    pr_data[\"recall\"],\n",
    "    pr_data[\"auprc\"],\n",
    "    title=\"ClinVar Pathogenicity - PR Curve\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = test_df[\"prediction\"].values\n",
    "\n",
    "fig = plot_confusion_matrix(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    class_names=[\"Benign\", \"Pathogenic\"],\n",
    "    title=\"ClinVar Pathogenicity - Confusion Matrix\",\n",
    "    normalize=False,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_calibration_curve(\n",
    "    y_true,\n",
    "    y_pred_proba,\n",
    "    n_bins=10,\n",
    "    title=\"ClinVar Model Calibration\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Overall distribution\n",
    "axes[0].hist(y_pred_proba, bins=50, alpha=0.7, edgecolor=\"black\")\n",
    "axes[0].axvline(x=THRESHOLD, color=\"r\", linestyle=\"--\", lw=2, label=f\"Threshold={THRESHOLD}\")\n",
    "axes[0].set_xlabel(\"Predicted Probability\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"Prediction Distribution\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# By true class\n",
    "benign_proba = test_df[test_df[\"is_pathogenic\"] == 0][\"prediction_proba\"]\n",
    "pathogenic_proba = test_df[test_df[\"is_pathogenic\"] == 1][\"prediction_proba\"]\n",
    "\n",
    "axes[1].hist(benign_proba, bins=30, alpha=0.6, label=\"Benign\", color=\"blue\")\n",
    "axes[1].hist(pathogenic_proba, bins=30, alpha=0.6, label=\"Pathogenic\", color=\"red\")\n",
    "axes[1].axvline(x=THRESHOLD, color=\"black\", linestyle=\"--\", lw=2, label=f\"Threshold={THRESHOLD}\")\n",
    "axes[1].set_xlabel(\"Predicted Probability\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].set_title(\"Prediction Distribution by True Class\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False positives\n",
    "false_positives = test_df[(test_df[\"is_pathogenic\"] == 0) & (test_df[\"prediction\"] == 1)]\n",
    "print(f\"\\n=== False Positives (n={len(false_positives)}) ===\")\n",
    "print(false_positives.nlargest(5, \"prediction_proba\")[[\"sequence\", \"is_pathogenic\", \"prediction_proba\"]])\n",
    "\n",
    "# False negatives\n",
    "false_negatives = test_df[(test_df[\"is_pathogenic\"] == 1) & (test_df[\"prediction\"] == 0)]\n",
    "print(f\"\\n=== False Negatives (n={len(false_negatives)}) ===\")\n",
    "print(false_negatives.nsmallest(5, \"prediction_proba\")[[\"sequence\", \"is_pathogenic\", \"prediction_proba\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different thresholds\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "threshold_metrics = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    metrics_t = compute_classification_metrics(y_true, y_pred_proba, threshold=thresh)\n",
    "    threshold_metrics.append({\n",
    "        \"threshold\": thresh,\n",
    "        \"accuracy\": metrics_t[\"accuracy\"],\n",
    "        \"f1\": metrics_t[\"f1\"],\n",
    "        \"precision\": metrics_t[\"precision\"],\n",
    "        \"sensitivity\": metrics_t[\"sensitivity\"],\n",
    "    })\n",
    "\n",
    "threshold_df = pd.DataFrame(threshold_metrics)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(threshold_df[\"threshold\"], threshold_df[\"accuracy\"], marker=\"o\", label=\"Accuracy\")\n",
    "ax.plot(threshold_df[\"threshold\"], threshold_df[\"f1\"], marker=\"s\", label=\"F1\")\n",
    "ax.plot(threshold_df[\"threshold\"], threshold_df[\"precision\"], marker=\"^\", label=\"Precision\")\n",
    "ax.plot(threshold_df[\"threshold\"], threshold_df[\"sensitivity\"], marker=\"v\", label=\"Sensitivity\")\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Metrics vs Classification Threshold\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "output_path = Path(\"../results/clinvar_predictions.csv\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "test_df.to_csv(output_path, index=False)\n",
    "print(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "# Save metrics\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "metrics_path = Path(\"../results/clinvar_metrics.csv\")\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "print(f\"Metrics saved to {metrics_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
