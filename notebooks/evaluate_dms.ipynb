{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Mutational Scanning (DMS) Evaluation\n",
    "\n",
    "This notebook evaluates the multitask CRISPR design model on held-out DMS test data.\n",
    "\n",
    "## Objectives\n",
    "1. Load trained model checkpoint\n",
    "2. Evaluate on held-out test set\n",
    "3. Compute regression metrics (MAE, RMSE, RÂ², Pearson/Spearman correlation)\n",
    "4. Visualize predictions vs ground truth\n",
    "5. Analyze error patterns and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "from crispr_design_agent.evaluation.metrics import (\n",
    "    compute_regression_metrics,\n",
    "    stratified_evaluation,\n",
    ")\n",
    "from crispr_design_agent.evaluation.visualization import (\n",
    "    plot_regression_results,\n",
    ")\n",
    "from crispr_design_agent.training.module import MultiTaskLightningModule\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"../models/checkpoints/multitask-epoch=10.ckpt\"\n",
    "DATA_PATH = \"../data/processed/dms.parquet\"\n",
    "VAL_SPLIT = 0.1\n",
    "SEED = 42\n",
    "MAX_LENGTH = 1024\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(DATA_PATH)\n",
    "print(f\"Loaded {len(df)} DMS measurements\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/val\n",
    "df = df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
    "cutoff = int(len(df) * (1 - VAL_SPLIT))\n",
    "test_df = df.iloc[cutoff:].reset_index(drop=True)\n",
    "\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(test_df[\"effect\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "model = MultiTaskLightningModule.load_from_checkpoint(CHECKPOINT_PATH, strict=False)\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model.encoder_name, trust_remote_code=True)\n",
    "\n",
    "print(f\"Model loaded from {CHECKPOINT_PATH}\")\n",
    "print(f\"Encoder: {model.encoder_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "batch_size = 8\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i in range(0, len(test_df), batch_size):\n",
    "        batch_df = test_df.iloc[i : i + batch_size]\n",
    "        sequences = batch_df[\"sequence\"].tolist()\n",
    "        \n",
    "        tokens = tokenizer(\n",
    "            sequences,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            padding=\"max_length\",\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        pooled = model.forward(tokens[\"input_ids\"], tokens[\"attention_mask\"])\n",
    "        logits = model.heads[\"dms\"](pooled).squeeze(-1)\n",
    "        \n",
    "        predictions.extend(logits.cpu().numpy().tolist())\n",
    "        \n",
    "        if (i + batch_size) % 100 == 0:\n",
    "            print(f\"Processed {min(i + batch_size, len(test_df))} / {len(test_df)}\")\n",
    "\n",
    "test_df[\"prediction\"] = predictions\n",
    "print(f\"\\nInference complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_df[\"effect\"].values\n",
    "y_pred = test_df[\"prediction\"].values\n",
    "\n",
    "metrics = compute_regression_metrics(y_true, y_pred)\n",
    "\n",
    "print(\"\\n=== DMS Regression Metrics ===\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key:20s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_regression_results(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    title=\"DMS Effect Prediction\",\n",
    "    figsize=(15, 4),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute errors\n",
    "test_df[\"abs_error\"] = np.abs(test_df[\"effect\"] - test_df[\"prediction\"])\n",
    "\n",
    "# Find worst predictions\n",
    "worst_predictions = test_df.nlargest(10, \"abs_error\")\n",
    "\n",
    "print(\"\\n=== Top 10 Worst Predictions ===\")\n",
    "print(worst_predictions[[\"sequence\", \"effect\", \"prediction\", \"abs_error\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error by sequence length\n",
    "test_df[\"seq_length\"] = test_df[\"sequence\"].str.len()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(test_df[\"seq_length\"], test_df[\"abs_error\"], alpha=0.5, s=10)\n",
    "ax.set_xlabel(\"Sequence Length\")\n",
    "ax.set_ylabel(\"Absolute Error\")\n",
    "ax.set_title(\"Prediction Error vs Sequence Length\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If dataset has protein/gene identifiers, evaluate per protein\n",
    "if \"protein_id\" in test_df.columns:\n",
    "    stratified_metrics = stratified_evaluation(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        test_df[\"protein_id\"].values,\n",
    "        problem_type=\"regression\",\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Per-Protein Metrics ===\")\n",
    "    for protein_id, metrics in stratified_metrics.items():\n",
    "        print(f\"\\n{protein_id}:\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "else:\n",
    "    print(\"No protein_id column found for stratified evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "output_path = Path(\"../results/dms_predictions.csv\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "test_df.to_csv(output_path, index=False)\n",
    "print(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "# Save metrics\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "metrics_path = Path(\"../results/dms_metrics.csv\")\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "print(f\"Metrics saved to {metrics_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
